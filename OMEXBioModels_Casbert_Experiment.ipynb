{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a434014c-6d48-460a-96ed-e2c57cbad550",
   "metadata": {},
   "source": [
    "# Experiment: Composite Annotation Search Using BERT for OMEX BioModels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0098af6-40a8-492f-b903-4254868b73ef",
   "metadata": {},
   "source": [
    "### Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698892f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sentence_transformers import util\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d775c2",
   "metadata": {},
   "source": [
    "### LOAD Required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of entities\n",
    "with open('casbert_resources/omex_entities.json', 'r') as fp:\n",
    "    entities = json.load(fp)\n",
    "    \n",
    "# Dict of entity embeddings\n",
    "entityEmbeddings = torch.load('casbert_resources/omex_entities.pt')\n",
    "\n",
    "# Dict of predicate embeddings\n",
    "predicates = torch.load('casbert_resources/omex_predicates.pt')\n",
    "\n",
    "# Dict of ontology class embedding\n",
    "classes = torch.load('casbert_resources/omex_classes.pt')\n",
    "\n",
    "# Query Set for test data\n",
    "with open('casbert_resources/omex_queryTest.json', 'r') as fp:\n",
    "    queryTest = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTModel = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "_model = SentenceTransformer(BERTModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6a37a",
   "metadata": {},
   "source": [
    "### Performance measure using Mean average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagePrecision(prediction):\n",
    "    if 1 not in prediction:\n",
    "        return 0\n",
    "    tot = 0\n",
    "    for idx, p in enumerate(prediction):\n",
    "        if p>0:\n",
    "            tot += sum(prediction[0:idx+1])/(idx+1)\n",
    "    return tot/sum(prediction)\n",
    "\n",
    "def meanAP(predictions):\n",
    "    tap = 0\n",
    "    stat = {}\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        ap = averagePrecision(prediction)\n",
    "        stat[idx] = ap\n",
    "        tap += ap\n",
    "    return {'score':tap/len(predictions), 'stat':stat}\n",
    "\n",
    "def meanRR(predictions):\n",
    "    trr = 0\n",
    "    stat = {}\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        rr = 1/(prediction.index(1)+1) if 1 in prediction else 0\n",
    "        stat[idx] = rr\n",
    "        trr += rr\n",
    "    return {'score':trr/len(predictions), 'stat':stat}\n",
    "\n",
    "def getMAP(queries, searchFunction, indexType=None, pathType=None, topK=10, minSim=0.5):\n",
    "    predictions = []\n",
    "    for query, facts in tqdm(queries.items()):\n",
    "        results = searchFunction(query=query, topK=topK, indexType=indexType, pathType=pathType, minSim=minSim)\n",
    "        predictions += [[1 if varId in facts['vars'] else 0 for varId in results]]\n",
    "    MAP = meanAP(predictions)\n",
    "    return {'MAP':MAP,'MRR':meanRR(predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad240b1",
   "metadata": {},
   "source": [
    "### Searching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b9b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some functions, we utilise scispacy to locate phrases or concepts related to a query. \n",
    "# For example a query 'Calcium reverse membrane potential.' \n",
    "# is identified having 2 concepts of 'calcium' and 'reverse membrane potential'\n",
    "# For accurate identification we use 'en_core_sci_scibert' which required GPU for faster performance\n",
    "\n",
    "import en_core_sci_scibert\n",
    "nlp = en_core_sci_scibert.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entitySearch(query, topK=20, indexType='class', pathType=None, minSim=None):\n",
    "    \"\"\"\n",
    "    In this approach:\n",
    "    1. Get vector of query\n",
    "    2. Get similar entities using cosine similarity\n",
    "    3. Return topK result in descending\n",
    "    \"\"\"\n",
    "    textEmbedding = _model.encode(query, convert_to_tensor=True)\n",
    "    # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "    cosScores = util.pytorch_cos_sim(textEmbedding, entityEmbeddings[indexType])[0]\n",
    "    topResults = torch.topk(cosScores, k=topK)\n",
    "    results = {}\n",
    "    varIds = entityEmbeddings['entityIds']\n",
    "    for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "        results[varIds[idx]] = [rank, score.item(), entities[varIds[idx]]['path']]\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49363f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pEmbeddings = torch.stack(list(predicates.values()))\n",
    "cEmbeddings = torch.stack([v['embedding'] for v in classes.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredicates(text, topK = 20):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        textEmbedding = _model.encode(text, convert_to_tensor=True)\n",
    "        # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "        cosScores = util.pytorch_cos_sim(textEmbedding, pEmbeddings)[0]\n",
    "        topResults = torch.topk(cosScores, k=topK)\n",
    "        p = {}\n",
    "        for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "            p[list(predicates.keys())[idx]] = (rank, score.item())\n",
    "        return p\n",
    "    \n",
    "def getClasses(text, feature='name_synonym', topK = 20):\n",
    "        \"\"\"\n",
    "        feature: name, name_synonym, name_synonym_def, name_synonym_def, name_synonym_def_parent\n",
    "        \"\"\"\n",
    "        textEmbedding = _model.encode(text, convert_to_tensor=True)\n",
    "        # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "        cosScores = util.pytorch_cos_sim(textEmbedding, cEmbeddings)[0]\n",
    "        topResults = torch.topk(cosScores, k=topK)\n",
    "        c = {}\n",
    "        for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "            classId = list(classes.keys())[idx]\n",
    "            c[classId] = (rank, score.item(), classes[classId]['text'][0])\n",
    "        return c\n",
    "        \n",
    "\n",
    "def entitySearchClass(query, topK=20, minSim=0.5, indexType='class', pathType=None):\n",
    "    \"\"\"\n",
    "    In this approach:\n",
    "    1. Query is chunked into entities and classified into biomedical phrases and predicate\n",
    "    2. Connect predicate to biomedical phrase\n",
    "    3. If indexType is 'class', generate vector for biomedical phrase\n",
    "    4. If indexType is 'class_predicate', generate vector for biomedical phrase and predicate pair\n",
    "    5. Combine vectors becoming one vector using mean function\n",
    "    6. Get similar entities encoding class_predicate using cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(query)\n",
    "    alpha = 1\n",
    "    \n",
    "    \n",
    "    ontoClasses = []\n",
    "    predicates = []\n",
    "    validClassPredicates = {}\n",
    "    offset2Class = {}\n",
    "    for ent in doc.ents:\n",
    "        predicateScores = getPredicates(ent.text, topK=1)\n",
    "        pScore = list(predicateScores.values())[0][1]\n",
    "        classScores = getClasses(ent.text, topK=1)\n",
    "        cScore = list(classScores.values())[0][1]\n",
    "        if cScore >= pScore:\n",
    "            ontoClasses += [ent]\n",
    "            for token in ent:\n",
    "                offset2Class[token.i] = ent\n",
    "        elif indexType=='class_predicate' and pScore >= minSim:\n",
    "            predicates += [ent]\n",
    "    \n",
    "    if len(ontoClasses)==0: \n",
    "        ontoClasses = [doc]\n",
    "    \n",
    "    \n",
    "    # check the entities describe by predicate (usually predicate's child)\n",
    "    for ent in predicates:\n",
    "        for token in ent:\n",
    "            for child in token.children:\n",
    "                if child.i in offset2Class:\n",
    "                    idx = ontoClasses.index(offset2Class[child.i])\n",
    "                    if idx not in validClassPredicates:\n",
    "                        validClassPredicates[idx] = [ent]\n",
    "                    else:\n",
    "                        validClassPredicates[idx] += [ent]\n",
    "                    break # only consider the closest class. remove break if considering all classes\n",
    "                \n",
    "    if indexType=='class':\n",
    "        classEmbeddings = [_model.encode(ent.text, convert_to_tensor=True) for ent in ontoClasses]\n",
    "        textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)\n",
    "    elif indexType=='class_predicate':\n",
    "        classEmbeddings = []\n",
    "        for i in range(len(ontoClasses)):\n",
    "            classEmbedding = _model.encode(ontoClasses[i].text, convert_to_tensor=True)\n",
    "            if i in validClassPredicates:\n",
    "                predicateEmbeddings = [_model.encode(ent.text, convert_to_tensor=True) for ent in validClassPredicates[i]]\n",
    "                pathEmbedding = alpha * torch.mean(torch.stack(predicateEmbeddings, dim=0), dim=0)\n",
    "                classEmbedding = torch.mean(torch.stack([classEmbedding, pathEmbedding], dim=0), dim=0)\n",
    "            classEmbeddings += [classEmbedding]\n",
    "        textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)                                \n",
    "        \n",
    "    # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "    cosScores = util.pytorch_cos_sim(textEmbedding, entityEmbeddings[indexType])[0]\n",
    "    topResults = torch.topk(cosScores, k=topK)\n",
    "    results = {}\n",
    "    varIds = entityEmbeddings['entityIds']\n",
    "    for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "        results[varIds[idx]] = [rank, score.item(), entities[varIds[idx]]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4245cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entitySearchCombine(query, topK=20, minSim=0.5, indexType='class', pathType=None):\n",
    "    \"\"\"\n",
    "    In this approach: combining entitySearch and entitySearchClass\n",
    "    1. Query is chunked into entities and classified into biomedical phrases and predicate\n",
    "    2. Connect predicate to biomedical phrase\n",
    "    3. If indexType is 'class', generate vector for biomedical phrase\n",
    "    4. If indexType is 'class_predicate', generate vector for biomedical phrase and predicate pair\n",
    "    5. Combine vectors becoming one vector using mean function, named it as local vector\n",
    "    6. Get vector of query named it as global vector\n",
    "    7. Combine local vector and global vector\n",
    "    8. Get similar entities using cosine similarity\n",
    "    9. Return topK result in descending\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get local query embedding \n",
    "    \n",
    "    doc = nlp(query)\n",
    "    alpha = 1\n",
    "    \n",
    "    ontoClasses = []\n",
    "    predicates = []\n",
    "    validClassPredicates = {}\n",
    "    offset2Class = {}\n",
    "    cScores = []\n",
    "    for ent in doc.ents:\n",
    "        predicateScores = getPredicates(ent.text, topK=1)\n",
    "        pScore = list(predicateScores.values())[0][1]\n",
    "        classScores = getClasses(ent.text, topK=1)\n",
    "        cScore = list(classScores.values())[0][1]\n",
    "        if cScore >= pScore:\n",
    "            cScores += [cScore]\n",
    "            ontoClasses += [ent]\n",
    "            for token in ent:\n",
    "                offset2Class[token.i] = ent\n",
    "        elif indexType =='class_predicate' and pScore >= minSim:\n",
    "            predicates += [ent]\n",
    "    \n",
    "    if len(ontoClasses) == 0: \n",
    "        ontoClasses = [doc]\n",
    "        cScores = [list(getClasses(query, topK=1).values())[0][1]]\n",
    "            \n",
    "    # check the entities describe by predicate (usually predicate's child)\n",
    "    for ent in predicates:\n",
    "        for token in ent:\n",
    "            for child in token.children:\n",
    "                if child.i in offset2Class:\n",
    "                    idx = ontoClasses.index(offset2Class[child.i])\n",
    "                    if idx not in validClassPredicates:\n",
    "                        validClassPredicates[idx] = [ent]\n",
    "                    else:\n",
    "                        validClassPredicates[idx] += [ent]\n",
    "                    break # only consider the closest class. remove break if considering all classes\n",
    "                \n",
    "    if indexType=='class':\n",
    "        classEmbeddings = [_model.encode(ent.text, convert_to_tensor=True) for ent in ontoClasses]\n",
    "        textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)\n",
    "    elif indexType=='class_predicate':\n",
    "        classEmbeddings = []\n",
    "        for i in range(len(ontoClasses)):\n",
    "            classEmbedding = _model.encode(ontoClasses[i].text, convert_to_tensor=True)\n",
    "            if i in validClassPredicates:\n",
    "                predicateEmbeddings = [_model.encode(ent.text, convert_to_tensor=True) for ent in validClassPredicates[i]]\n",
    "                pathEmbedding = alpha * torch.mean(torch.stack(predicateEmbeddings, dim=0), dim=0)\n",
    "                classEmbedding = torch.mean(torch.stack([classEmbedding, pathEmbedding], dim=0), dim=0)\n",
    "            classEmbeddings += [classEmbedding]\n",
    "        textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)                                \n",
    "        \n",
    "    ### Get global query embedding\n",
    "    textEmbeddingGlobal = _model.encode(query, convert_to_tensor=True)\n",
    "    ### Combine global and local embedding\n",
    "    factor = sum(cScores)/len(cScores)\n",
    "    textEmbedding = torch.mean(torch.stack([textEmbeddingGlobal, factor * textEmbedding], dim=0), dim=0)       \n",
    "    \n",
    "    # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "    cosScores = util.pytorch_cos_sim(textEmbedding, entityEmbeddings[indexType])[0]\n",
    "    topResults = torch.topk(cosScores, k=topK)\n",
    "    results = {}\n",
    "    varIds = entityEmbeddings['entityIds']\n",
    "    for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "        results[varIds[idx]] = [rank, score.item(), entities[varIds[idx]]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is experimenting with predicate use decided using classifier\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "modelClassifier = AutoModelForSequenceClassification.from_pretrained('casbert_resources/omex_trained_model', num_labels=2)\n",
    "\n",
    "def classifyQuery(model, sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = modelClassifier(**inputs)\n",
    "    return outputs.logits.argmax(dim=-1)[0]\n",
    "\n",
    "def entitySearchCombine2(query, topK=20, minSim=0.5, indexType=None, pathType=None):\n",
    "    \"\"\"\n",
    "    In this approach: combining entitySearch and entitySearchClass\n",
    "    1. Query is chunked into entities and classified into biomedical phrases and predicate\n",
    "    2. Connect predicate to biomedical phrase\n",
    "    3. Here, the query is classify into 'class' or 'class_predicate'\n",
    "        a. If indexType is 'class', generate vector for biomedical phrase\n",
    "        b. If indexType is 'class_predicate', generate vector for biomedical phrase and predicate pair\n",
    "    4. Combine vectors becoming one vector using mean function, named it as local vector\n",
    "    5. Get vector of query named it as global vector\n",
    "    6. Combine local vector and global vector\n",
    "    7. Get similar entities using cosine similarity\n",
    "    8. Return topK result in descending\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get local query embedding \n",
    "    \n",
    "    doc = nlp(query)\n",
    "    \n",
    "    ontoClasses = []\n",
    "    predicates = []\n",
    "    validClassPredicates = {}\n",
    "    offset2Class = {}\n",
    "    cScores = []\n",
    "    for ent in doc.ents:\n",
    "        \n",
    "        predicateScores = getPredicates(ent.text, topK=1)\n",
    "        pScore = list(predicateScores.values())[0][1]\n",
    "        classScores = getClasses(ent.text, topK=1)\n",
    "        cScore = list(classScores.values())[0][1]\n",
    "        if cScore >= pScore:\n",
    "            cScores += [cScore]\n",
    "            ontoClasses += [ent]\n",
    "            for token in ent:\n",
    "                offset2Class[token.i] = ent\n",
    "        else:\n",
    "            predicates += [ent]\n",
    "    \n",
    "    if len(ontoClasses) == 0: \n",
    "        ontoClasses = [doc]\n",
    "        cScores = [list(getClasses(query, topK=1).values())[0][1]]\n",
    "            \n",
    "    # check the entities describe by predicate (usually predicate's child)\n",
    "    for ent in predicates:\n",
    "        for token in ent:\n",
    "            for child in token.children:\n",
    "                if child.i in offset2Class:\n",
    "                    idx = ontoClasses.index(offset2Class[child.i])\n",
    "                    if idx not in validClassPredicates:\n",
    "                        validClassPredicates[idx] = [ent]\n",
    "                    else:\n",
    "                        validClassPredicates[idx] += [ent]\n",
    "                    break # only consider the closest class. remove break if considering all classes\n",
    "    \n",
    "    classEmbeddings = []\n",
    "    for i in range(len(ontoClasses)):\n",
    "        classEmbedding = _model.encode(ontoClasses[i].text, convert_to_tensor=True)\n",
    "        if i in validClassPredicates:\n",
    "            predicateEmbeddings = [_model.encode(ent.text, convert_to_tensor=True) for ent in validClassPredicates[i]]\n",
    "            #pathEmbedding = alpha * torch.mean(torch.stack(predicateEmbeddings, dim=0), dim=0)\n",
    "            pathEmbedding = torch.mean(torch.stack(predicateEmbeddings, dim=0), dim=0)\n",
    "            classEmbedding = torch.mean(torch.stack([classEmbedding, pathEmbedding], dim=0), dim=0)\n",
    "        classEmbeddings += [classEmbedding]\n",
    "    textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)      \n",
    "        \n",
    "    ### Get global query embedding\n",
    "    textEmbeddingGlobal = _model.encode(query, convert_to_tensor=True)\n",
    "    ### Combine global and local embedding\n",
    "    factor = sum(cScores)/len(cScores)\n",
    "    textEmbedding = torch.mean(torch.stack([textEmbeddingGlobal, factor * textEmbedding], dim=0), dim=0)       \n",
    "    \n",
    "    # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "    bestType = classifyQuery(modelClassifier, query)\n",
    "    if bestType == 0:\n",
    "        cosScores = util.pytorch_cos_sim(textEmbedding, entityEmbeddings['class'])[0]\n",
    "    else:\n",
    "        cosScores = util.pytorch_cos_sim(textEmbedding, entityEmbeddings['class_predicate'])[0]\n",
    "    \n",
    "    topResults = torch.topk(cosScores, k=topK)\n",
    "    results = {}\n",
    "    varIds = entityEmbeddings['entityIds']\n",
    "    for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "        results[varIds[idx]] = [rank, score.item(), entities[varIds[idx]]]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb345b39",
   "metadata": {},
   "source": [
    "### Now check for TF-IDF BM-25, for ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cfe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### organised variable text as a plain text then store it into dataframe\n",
    "\n",
    "# loop for each variable\n",
    "varTexts = {'varId':[], 'name':[], 'name_synonym':[]}\n",
    "for varId in entityEmbeddings['entityIds']:\n",
    "    value = entities[varId]\n",
    "    names = []\n",
    "    \n",
    "    for leaf in value['object']:\n",
    "        if leaf in classes:\n",
    "            names += classes[leaf]['text']\n",
    "\n",
    "    if len(names)==0: continue\n",
    "    varTexts['varId'] += [varId]\n",
    "\n",
    "    varTexts['name'] += [', '.join(set(names[:1]))]\n",
    "    varTexts['name_synonym'] += [', '.join(set(names))]\n",
    "\n",
    "dfVarTexts = pd.DataFrame(varTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ee94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation of OKapi BM25 with sklearn's TfidfVectorizer\n",
    "Distributed as CC-0 (https://creativecommons.org/publicdomain/zero/1.0/)\n",
    "\"\"\"\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    def __init__(self, b=0.75, k1=1.6):\n",
    "        self.vectorizer = TfidfVectorizer(norm=None, smooth_idf=False)\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        self.vectorizer.fit(X)\n",
    "        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        # apply CountVectorizer\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        # convert to csc for better column slicing\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        # idf(t) = log [ n / df(t) ] + 1 in sklearn, so it need to be coneverted\n",
    "        # to idf(t) = log [ n / df(t) ] with minus 1\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1\n",
    "\n",
    "\n",
    "\n",
    "#------------ End of library impl. Followings are the example -----------------\n",
    "bm25 = {}\n",
    "for col in dfVarTexts.columns[1:]:\n",
    "    bm25[col] = BM25()\n",
    "    bm25[col].fit(dfVarTexts[col])\n",
    "    \n",
    "\n",
    "def entitySearchBM25(query, topK=20, indexType='name_synonym', pathType='single', minSim=0):\n",
    "    \"\"\"\n",
    "    In this approach:\n",
    "    Simply using bag of word method BM25\n",
    "    \"\"\"\n",
    "    results = bm25[indexType].transform(query, dfVarTexts[indexType])\n",
    "\n",
    "    sortedResults = {}\n",
    "    for i in results.argsort()[-topK:][::-1]:\n",
    "        sortedResults[dfVarTexts.iloc[i,0]] = (results[i], dfVarTexts.iloc[i,1])\n",
    "        \n",
    "    return sortedResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1d8a7",
   "metadata": {},
   "source": [
    "## Now run to measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'noPredicate':{}, 'withPredicate':{}, 'combine':{}}\n",
    "topK = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa49c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataSetType = 'noPredicate'\n",
    "querySet = queryTest[dataSetType]\n",
    "results[dataSetType]['macro'] = getMAP(querySet, entitySearch, topK=topK, indexType='class')\n",
    "results[dataSetType]['macroWP'] = getMAP(querySet, entitySearch, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['micro'] = getMAP(querySet, entitySearchClass, topK=topK, indexType='class')\n",
    "results[dataSetType]['microWP'] = getMAP(querySet, entitySearchClass, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['mixed'] = getMAP(querySet, entitySearchCombine, topK=topK, indexType='class')\n",
    "results[dataSetType]['mixedWP'] = getMAP(querySet, entitySearchCombine, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['mixedCl'] = getMAP(querySet, entitySearchCombine2, topK=topK)\n",
    "results[dataSetType]['BM25'] = getMAP(querySet, entitySearchBM25, topK=topK, indexType='name_synonym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83696d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataSetType = 'withPredicate'\n",
    "querySet = queryTest[dataSetType]\n",
    "results[dataSetType]['macro'] = getMAP(querySet, entitySearch, topK=topK, indexType='class')\n",
    "results[dataSetType]['macroWP'] = getMAP(querySet, entitySearch, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['micro'] = getMAP(querySet, entitySearchClass, topK=topK, indexType='class')\n",
    "results[dataSetType]['microWP'] = getMAP(querySet, entitySearchClass, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['mixed'] = getMAP(querySet, entitySearchCombine, topK=topK, indexType='class')\n",
    "results[dataSetType]['mixedWP'] = getMAP(querySet, entitySearchCombine, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['mixedCl'] = getMAP(querySet, entitySearchCombine2, topK=topK)\n",
    "results[dataSetType]['BM25'] = getMAP(querySet, entitySearchBM25, topK=topK, indexType='name_synonym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetType = 'combine'\n",
    "querySet = queryTest[dataSetType]\n",
    "results[dataSetType]['macro'] = getMAP(querySet, entitySearch, topK=topK, indexType='class')\n",
    "results[dataSetType]['macroWP'] = getMAP(querySet, entitySearch, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['micro'] = getMAP(querySet, entitySearchClass, topK=topK, indexType='class')\n",
    "results[dataSetType]['microWP'] = getMAP(querySet, entitySearchClass, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['mixed'] = getMAP(querySet, entitySearchCombine, topK=topK, indexType='class')\n",
    "results[dataSetType]['mixedWP'] = getMAP(querySet, entitySearchCombine, topK=topK, indexType='class_predicate')\n",
    "results[dataSetType]['mixedCl'] = getMAP(querySet, entitySearchCombine2, topK=topK)\n",
    "results[dataSetType]['BM25'] = getMAP(querySet, entitySearchBM25, topK=topK, indexType='name_synonym')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28157ee6",
   "metadata": {},
   "source": [
    "### Analyse result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4afde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalResults = {'approach':[], 'noPredicate_MAP':[], 'noPredicate_MRR':[], 'withPredicate_MAP':[], 'withPredicate_MRR':[], 'combine_MAP':[], 'combine_MRR':[]}\n",
    "\n",
    "for approach in list(results.values())[0].keys():\n",
    "    globalResults['approach'] += [approach]\n",
    "    for dataType, v in results.items():\n",
    "        globalResults[dataType+'_MAP'] += [v[approach]['MAP']['score']]\n",
    "        globalResults[dataType+'_MRR'] += [v[approach]['MRR']['score']]\n",
    "\n",
    "df_globalResults = pd.DataFrame(globalResults)\n",
    "df_globalResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailResults = {'approach':[], 'dataType':[], 'query_Class_Sim':[], 'MAP':[], 'MRR':[]}\n",
    "queryReference = {'noPredicate':[v['score'] for k,v in queryTest['noPredicate'].items()], \n",
    "                  'withPredicate':[v['score'] for k,v in queryTest['withPredicate'].items()], \n",
    "                  'combine':[v['score'] for k,v in queryTest['combine'].items()]} \n",
    "\n",
    "for approach in list(results.values())[0].keys():\n",
    "    for dataType, v in results.items():\n",
    "        for a,b,c in zip(queryReference[dataType], list(v[approach]['MAP']['stat'].values()), list(v[approach]['MRR']['stat'].values())):\n",
    "            detailResults['approach'] += [approach]\n",
    "            detailResults['dataType'] += [dataType]\n",
    "            detailResults['query_Class_Sim'] += [a]\n",
    "            detailResults['MAP'] += [b]\n",
    "            detailResults['MRR'] += [c]\n",
    "\n",
    "        \n",
    "#         detailResults[dataType+'_MAP'] += [v[approach]['MAP']['score']]\n",
    "#         detailResults[dataType+'_MRR'] += [v[approach]['MRR']['score']]\n",
    "\n",
    "df_detailResults = pd.DataFrame(detailResults)\n",
    "df_detailResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a181487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfPlot = df_detailResults[df_detailResults['dataType']=='combine']\n",
    "dfPlot.query_Class_Sim = np.floor(dfPlot.query_Class_Sim*10)/10\n",
    "dfPlot.loc[(dfPlot.query_Class_Sim == 1.0),'query_Class_Sim']=0.9\n",
    "\n",
    "dfPlot.query_Class_Sim = dfPlot.query_Class_Sim.astype(str)+'-'+(dfPlot.query_Class_Sim+0.1).round(1).astype(str)\n",
    "# dfPlot = dfPlot.groupby(['approach', 'query_Class_Sim'], as_index=False)['MAP', 'MRR'].mean()\n",
    "dfPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ax = sns.lineplot(x = \"query_Class_Sim\", y = \"MAP\", hue=\"approach\", data = dfPlot)\n",
    "order = dfPlot['query_Class_Sim'].unique().tolist()\n",
    "order.sort()\n",
    "ax = sns.factorplot(x = \"query_Class_Sim\", y = \"MAP\", hue=\"approach\", data = dfPlot, order=order, legend=False)\n",
    "ax.despine(left=True)\n",
    "plt.legend(loc='lower right')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches( 10, 8)\n",
    "ax.set(xlabel=\"'Query - Entity's Terms' Similarity\", ylabel=\"mAP@10\")\n",
    "# ax.axes\n",
    "\n",
    "counts = tuple((100.*dfPlot.groupby('query_Class_Sim').count()['approach']/dfPlot.shape[0]).round(1).astype(str)+'%')\n",
    "for i, c in enumerate(counts):\n",
    "    plt.annotate(c, xy = (0.+i*0.92, 0.5+i*0.15), \n",
    "             fontsize = 8, xytext = (0.+i*0.9, 0.5+i*0.16), \n",
    "             color = 'g')\n",
    "\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6f548-b8f9-4131-9fae-419d66a40588",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('casbert_results/omex_results.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c7190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
