{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1dd0dcd",
   "metadata": {},
   "source": [
    "# Preparing data and indexing to be used for experiment using PMR repositories\n",
    "    We are using a file consisting of annotated entities in the PMR repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required modules\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b3a50",
   "metadata": {},
   "source": [
    "## Create dictionaries of ontology classes, predicates, and entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data of variables, components, cellms (already extracted from repositories)\n",
    "\n",
    "with open('casbert_resources/pmr_list_of_variable.json', 'r') as fp:\n",
    "    variables = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc98b9",
   "metadata": {},
   "source": [
    "#### Extract predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get set of predicates and ontology classes from repositories\n",
    "# besides ontology classes, there are also other information such as reference and textual information\n",
    "\n",
    "predicates = []\n",
    "leave = {'file':[], 'http':[], 'other':[]}\n",
    "for key, data in variables['data'].items():\n",
    "    if 'rdf' in data:\n",
    "        for triple in data['rdf']:\n",
    "            predicates += [triple[1]]\n",
    "\n",
    "# set of predicates\n",
    "predicates = set(predicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162cf91e",
   "metadata": {},
   "source": [
    "#### Extract relationship b/w ontology classes and entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class2Vars = {}\n",
    "\n",
    "for varId, value in variables['data'].items():\n",
    "    \n",
    "    if 'rdfLeaves' in value:\n",
    "        for leaf in value['rdfLeaves']:\n",
    "            leaf = str(leaf).strip()\n",
    "            # get ontology class para\n",
    "            classId = None\n",
    "            if leaf.startswith('http'):\n",
    "                classId = leaf.rsplit('/',1)[-1].rsplit('#',1)[-1].replace('_',':')\n",
    "            elif leaf.startswith('urn:miriam'):\n",
    "                classId = leaf.rsplit(':',1)[-1].replace('%3A',':')\n",
    "            if classId != None:\n",
    "                if classId not in class2Vars:\n",
    "                    class2Vars[classId] = []\n",
    "                class2Vars[classId] += [varId]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ecb66",
   "metadata": {},
   "source": [
    "#### Preparing embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "\n",
    "# BERT model used\n",
    "modelName = 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
    "seqLength = 128\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "word_embedding_model = models.Transformer(modelName, max_seq_length=seqLength)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "sbert = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14f2ea",
   "metadata": {},
   "source": [
    "#### Load ontology terms from available files\n",
    "Ontologies:\n",
    "  - OPB\n",
    "  - CL\n",
    "  - FMA\n",
    "  - GO\n",
    "  - CHEBI\n",
    "  - PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08596924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ontology dictionaries\n",
    "import gzip, pickle\n",
    "file = gzip.GzipFile('casbert_resources/ontoDf.gz', 'rb')\n",
    "ontologies = pickle.load(file)\n",
    "file.close()\n",
    "ontologies.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7a493",
   "metadata": {},
   "source": [
    "#### Create dict of ontoly class embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc95093",
   "metadata": {},
   "outputs": [],
   "source": [
    "classId_List = []\n",
    "ontoEmbedding = {'name':[], 'synonym':[], 'name_synonym':[], 'def':[], 'name_synonym_def':[], 'name_synonym_def':[]}\n",
    "embedShape = 384\n",
    "\n",
    "def change2embedding(txt):\n",
    "    return sbert.encode(txt, convert_to_tensor=True)\n",
    "\n",
    "def from_synonym(syn):\n",
    "    tmp = []\n",
    "    try:\n",
    "        for txt in syn.split('|'):\n",
    "            if len(txt) == 0: continue\n",
    "            if (txt.find('\"') < txt.rfind('\"')):\n",
    "                tmp += [txt[txt.find('\"')+1:txt.rfind('\"')]]\n",
    "            elif (txt.find(\"'\") == txt.rfind(\"'\")):\n",
    "                tmp += [txt[txt.find(\"\\'\")+1:txt.rfind(\"\\'\")]]\n",
    "            else: \n",
    "                tmp += [txt] \n",
    "        return torch.mean(change2embedding(tmp), 0)\n",
    "    except:\n",
    "        return torch.zeros(embedShape)\n",
    "\n",
    "for classId in tqdm(class2Vars):\n",
    "    if classId in ontologies.index:\n",
    "        ocls = ontologies.loc[classId]\n",
    "        classId_List += [classId]\n",
    "        ontoEmbedding['name'] += [change2embedding(ocls['name'])]\n",
    "        ontoEmbedding['synonym'] += [from_synonym(ocls['synonym'])]\n",
    "        if isinstance(ocls['def'], str):\n",
    "            ontoEmbedding['def'] += [change2embedding(ocls['def'])]\n",
    "        else:\n",
    "            ontoEmbedding['def'] += [torch.zeros(embedShape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5affe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontoEmbedding['name'] = torch.stack(ontoEmbedding['name'], dim=0)\n",
    "ontoEmbedding['synonym'] = torch.stack(ontoEmbedding['synonym'], dim=0)\n",
    "ontoEmbedding['def'] = torch.stack(ontoEmbedding['def'], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f22c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now combine some embeddings\n",
    "\n",
    "# name_synonym\n",
    "tensor = torch.stack([ontoEmbedding['name'], ontoEmbedding['synonym']], dim=0)\n",
    "ontoEmbedding['name_synonym'] = torch.div(torch.nansum(tensor, dim=0),(~torch.isnan(tensor)).count_nonzero(dim=0))\n",
    "\n",
    "# name_synonym_def\n",
    "tensor = torch.stack([ontoEmbedding['name'], ontoEmbedding['synonym'], ontoEmbedding['def']], dim=0)\n",
    "ontoEmbedding['name_synonym_def'] = torch.div(torch.nansum(tensor, dim=0),(~torch.isnan(tensor)).count_nonzero(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OntoEmbedding function\n",
    "from sentence_transformers import util\n",
    "    \n",
    "# returning text\n",
    "def getClassText(classId, features=['name']):\n",
    "    \"\"\"\n",
    "    features: name, synonym, parent, def\n",
    "    \"\"\"\n",
    "    corpus = {}\n",
    "    if classId in classId_List:\n",
    "        for feature in features:\n",
    "            corpus[feature] = ontologies.loc[classId][feature]\n",
    "    return corpus\n",
    "\n",
    "# returning embedding\n",
    "def getClassEmbedding(classId, feature='name_synonym'):\n",
    "    \"\"\"\n",
    "    classId: an id of a class such as 'CHEBI:29101'\n",
    "    feature: name, name_synonym, name_synonym_def, name_synonym_def, name_synonym_def_parent\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    if 'http' in classId: classId = classId.rsplit('/')[-1].split('#')[-1].replace('_',':')\n",
    "    if classId in classId_List:\n",
    "        return ontoEmbedding[feature][classId_List.index(classId)]\n",
    "    return None\n",
    "\n",
    "# returning ontology classes\n",
    "def getClasses(text, feature='name_synonym', topK = 20):\n",
    "    \"\"\"\n",
    "    feature: name, name_synonym, name_synonym_def, name_synonym_def, name_synonym_def_parent\n",
    "    \"\"\"\n",
    "    textEmbedding = sbert.encode(text, convert_to_tensor=True)\n",
    "    # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "    if feature in ontoEmbedding:\n",
    "        cosScores = util.pytorch_cos_sim(textEmbedding, ontoEmbedding[feature])[0]\n",
    "        topResults = torch.topk(cosScores, k=topK)\n",
    "        classes = {}\n",
    "        for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "            classId = classId_List[idx.item()]\n",
    "            classes[classId] = (rank, score.item(), ontologies.loc[classId]['name'])\n",
    "        return classes\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ontoEmbedding, 'casbert_resources/pmr_classes.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda33db",
   "metadata": {},
   "source": [
    "#### Create dictionary of predicate embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504eddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _camelCaseSplitter(identifier):\n",
    "    from re import finditer\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return ' '.join([m.group(0) for m in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "textPredicates = []\n",
    "purePredicates = []\n",
    "for p in predicates:\n",
    "    predicate = _camelCaseSplitter(p.split('#')[-1].split('/')[-1]) if p.startswith('http') else p\n",
    "    if len(predicate.split())>0 and len(predicate) > 2:\n",
    "        textPredicates += [predicate]\n",
    "        purePredicates += [p]\n",
    "predicateEmbedding = dict(zip(purePredicates, sbert.encode(textPredicates, convert_to_tensor=True, show_progress_bar=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredicateEmbedding(predicates):\n",
    "    \"\"\"\n",
    "    predicates: a string or a list of predicate. if it is a list, the mean of embedding is returned\n",
    "    \"\"\"\n",
    "    if type(predicates) == str: predicates = [predicates]\n",
    "    embeddings = []\n",
    "    for predicate in predicates:\n",
    "        if predicate in predicateEmbedding:\n",
    "            embeddings += [predicateEmbedding[predicate]]\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    return torch.mean(torch.stack(embeddings), dim=0)\n",
    "\n",
    "def getPredicates(text, topK = 20):\n",
    "    \"\"\"\n",
    "    text: string about predicate\n",
    "    returning a dictionary of predicates ordered by similarities\n",
    "    \"\"\"\n",
    "    textEmbedding = sbert.encode(text, convert_to_tensor=True)\n",
    "    # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "    cosScores = util.pytorch_cos_sim(textEmbedding, torch.stack(list(predicateEmbedding.values())))[0]\n",
    "    topResults = torch.topk(cosScores, k=topK)\n",
    "    predicates = {}\n",
    "    for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "        predicates[list(predicateEmbedding.keys())[idx]] = (rank, score.item())\n",
    "    return predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(predicateEmbedding, 'casbert_resources/pmr_predicates.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017e347",
   "metadata": {},
   "source": [
    "#### Create dictionary of entity embeddings\n",
    "This step includes:\n",
    "- creating entity embeddings\n",
    "- extracting query-entity dataset for testing candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b6a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get entity representation (ontology class and predicates - ontology classes)\n",
    "\n",
    "\n",
    "def getPredicatePaths(obj=None, prevPred=[], arrRdf=None):\n",
    "    import numpy as np\n",
    "    if obj != None:\n",
    "        rows = arrRdf[np.where(arrRdf[:,2] == obj)]\n",
    "    if len(rows) > 0:\n",
    "        finalPreds = []\n",
    "        for row in rows:\n",
    "            finalPreds += getPredicatePaths(obj=row[0], prevPred=[row[1]]+prevPred, arrRdf=arrRdf)\n",
    "        return list(finalPreds)\n",
    "    return [prevPred]\n",
    "\n",
    "test = {}  ## this if for dataset\n",
    "\n",
    "# classId to content (ontology classes and predicate paths)\n",
    "entityKeys = {}\n",
    "entityIds = []\n",
    "entityEmbedding = {'class':[], 'class_predicate':[]}\n",
    "\n",
    "# ontology class - predicate to varIds\n",
    "classPredicateMap = {'multi':{}, 'single':{}}\n",
    "# the embedding of ontology class - predicate\n",
    "classPredicateEmbedding = {'multi':[], 'single':[]}\n",
    "\n",
    "alpha = 0.22\n",
    "\n",
    "\n",
    "# loop for each variable\n",
    "for varId, value in variables['data'].items():\n",
    "    \n",
    "    embeddings = []\n",
    "    pathClassEmbeddings = []\n",
    "    terms = {}\n",
    "    candQuery = []\n",
    "    if 'rdfLeaves' in value:\n",
    "        for leaf in value['rdfLeaves']:\n",
    "            leaf = str(leaf).strip()\n",
    "            # get ontology class para\n",
    "            if leaf.startswith('http'):\n",
    "                classId = leaf.rsplit('/',1)[-1].rsplit('#',1)[-1].replace('_',':')\n",
    "            elif leaf.startswith('urn:miriam'):\n",
    "                classId = leaf.rsplit(':',1)[-1].replace('%3A',':')\n",
    "            if leaf.startswith('http') or leaf.startswith('urn:miriam'):\n",
    "                embedding = getClassEmbedding(classId)\n",
    "                # get ontology class embedding\n",
    "                if embedding != None:\n",
    "                    embeddings += [embedding]\n",
    "                    # get predicates path embedding\n",
    "                    import numpy as np\n",
    "                    arrRdf = np.array(value['rdf'])\n",
    "                    paths = getPredicatePaths(obj=leaf, arrRdf=arrRdf)\n",
    "                    pathTexts = [[p.rsplit('/')[-1].rsplit('#')[-1] for p in path] for path in paths]\n",
    "                    pathEmbeddings = [getPredicateEmbedding(path) for path in paths if getPredicateEmbedding(path) != None]\n",
    "                    if len(pathEmbeddings) > 0:\n",
    "                        pathEmbedding = alpha * torch.mean(torch.stack(pathEmbeddings, dim=0), dim=0)\n",
    "                        pathClassEmbedding = torch.sum(torch.stack([embedding, pathEmbedding], dim=0), dim=0)/(1+alpha)\n",
    "                        pathClassEmbeddings += [pathClassEmbedding]\n",
    "                    else:\n",
    "                        pathClassEmbeddings += [embedding]\n",
    "                    \n",
    "                    # setting for the use of unique classPredicateEmbedding\n",
    "                    for path in paths:\n",
    "                        key = leaf + '-'.join([p.rsplit('/')[-1].rsplit('#')[-1] for p in path])\n",
    "                        #index pair of path and ontology class\n",
    "                        pEmbedding = getPredicateEmbedding(path)\n",
    "                        if pEmbedding != None:\n",
    "                            tmpEmbedding = torch.mean(torch.stack([embedding, pEmbedding], dim=0), dim=0)\n",
    "                        else:\n",
    "                            tmpEmbedding = embedding\n",
    "                        if key not in classPredicateMap['multi']:\n",
    "                            classPredicateEmbedding['multi'] += [tmpEmbedding]\n",
    "                            classPredicateMap['multi'][key] = [varId]\n",
    "                        else:\n",
    "                            classPredicateMap['multi'][key] += [varId]\n",
    "                        \n",
    "                        # index single predicate and ontology class\n",
    "                        for predicate in path:\n",
    "                            key = leaf + predicate\n",
    "                            pEmbedding = getPredicateEmbedding([predicate])\n",
    "                            if pEmbedding != None:\n",
    "                                tmpEmbedding = torch.mean(torch.stack([embedding, pEmbedding], dim=0), dim=0)\n",
    "                            else:\n",
    "                                tmpEmbedding = embedding\n",
    "                            if key not in classPredicateMap['single']:\n",
    "                                classPredicateEmbedding['single'] += [tmpEmbedding]\n",
    "                                classPredicateMap['single'][key] = [varId]\n",
    "                            else:\n",
    "                                classPredicateMap['single'][key] += [varId]\n",
    "                        \n",
    "                    # set term\n",
    "                    terms[classId] = {'name':getClassText(classId, features=['name'])['name'], 'path':pathTexts}\n",
    "            elif not leaf.startswith('file://'):\n",
    "                paths = getPredicatePaths(obj=leaf, arrRdf=arrRdf)\n",
    "                for path in paths:\n",
    "                    if 'description' in ''.join(path):\n",
    "                        candQuery += [leaf]\n",
    "                        break\n",
    "    \n",
    "        if len(embeddings) > 0:\n",
    "            entityKeys[varId] = {'pos':len(entityKeys), 'classes':terms}\n",
    "            entityEmbedding['class'] += [torch.mean(torch.stack(embeddings, dim=0), dim=0)]\n",
    "            entityEmbedding['class_predicate'] += [torch.mean(torch.stack(pathClassEmbeddings, dim=0), dim=0)]\n",
    "            entityIds += [varId]\n",
    "            for q in candQuery:\n",
    "                if q not in test:\n",
    "                    test[q] = {}\n",
    "                test[q][varId] = terms\n",
    "                    \n",
    "\n",
    "for k, v in entityEmbedding.items():\n",
    "    entityEmbedding[k] = torch.stack(v, dim=0)\n",
    "\n",
    "classPredicateEmbedding['multi'] = torch.stack(classPredicateEmbedding['multi'], dim=0)\n",
    "classPredicateEmbedding['single'] = torch.stack(classPredicateEmbedding['single'], dim=0)\n",
    "    \n",
    "entityEmbedding['class'].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(entityEmbedding, 'casbert_resources/pmr_entities.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c17d5",
   "metadata": {},
   "source": [
    "## Performance measure using Mean average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6974206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagePrecision(prediction):\n",
    "    if 1 not in prediction:\n",
    "        return 0\n",
    "    tot = 0\n",
    "    for idx, p in enumerate(prediction):\n",
    "        if p>0:\n",
    "            tot += sum(prediction[0:idx+1])/(idx+1)\n",
    "    return tot/sum(prediction)\n",
    "\n",
    "def meanAP(predictions):\n",
    "    tap = 0\n",
    "    stat = {}\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        ap = averagePrecision(prediction)\n",
    "        stat[idx] = ap\n",
    "        tap += ap\n",
    "    return {'score':tap/len(predictions), 'stat':stat}\n",
    "\n",
    "def meanRR(predictions):\n",
    "    trr = 0\n",
    "    stat = {}\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        rr = 1/(prediction.index(1)+1) if 1 in prediction else 0\n",
    "        stat[idx] = rr\n",
    "        trr += rr\n",
    "    return {'score':trr/len(predictions), 'stat':stat}\n",
    "\n",
    "def getMAP(queries, searchFunction, indexType=None, pathType=None, topK=10, minSim=0.5):\n",
    "    predictions = []\n",
    "    for query, facts in tqdm(queries.items()):\n",
    "        results = searchFunction(query=query, topK=topK, indexType=indexType, pathType=pathType, minSim=minSim)\n",
    "        predictions += [[1 if varId in facts['vars'] else 0 for varId in results]]\n",
    "    MAP = meanAP(predictions)\n",
    "    return {'MAP':MAP,'MRR':meanRR(predictions)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getMAPs(queries, functions, indexTypes=['class', 'class_predicate'], pathTypes=['single', 'multi'], topK=10, minSim=0.5):\n",
    "    for idx, path in zip(indexTypes, pathTypes):\n",
    "        print(idx, path)\n",
    "    for function in functions:\n",
    "        print(function.__name__)\n",
    "#         print(inspect.getfullargspec(function))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45166e2",
   "metadata": {},
   "source": [
    "## Preparing Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the quality of query-entity pairs by their similarity:\n",
    "    # 0 if query-entity similarity > 0.7\n",
    "    # -1 if 0.5 <= query-entity similarity <= 0.7\n",
    "    # -2 if query-entity similarity < 0.5\n",
    "\n",
    "# save test data to csv, and then inspect manually by experts to validate\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "for query, classes in tqdm(test.items()):\n",
    "    queryEmb = sbert.encode(query, convert_to_tensor=True)\n",
    "    for varId, classes in classes.items():\n",
    "        if len(query) > 0:\n",
    "            entityEmb = entityEmbedding['class'][entityKeys[varId]['pos']]\n",
    "            cosScores = util.pytorch_cos_sim(entityEmb, queryEmb)\n",
    "            check = -1 if cosScores >= 0.5 and cosScores <= 0.7 else -2 if cosScores < 0.5 else 0\n",
    "            data += [[query, check, classes, varId, cosScores.item()]]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['query', 'check', 'classes', 'varID', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('casbert_resources/pmr_test_data_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9693e",
   "metadata": {},
   "source": [
    "#### Manual inspection is painful, but we did it\n",
    "It is available now with same name \"pmr_test_data.csv\"\n",
    "\n",
    "Columns: query, check, classes, varID, score, query_modif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('casbert_resources/pmr_test_data.csv')\n",
    "df['query'].unique().shape\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c8b7e",
   "metadata": {},
   "source": [
    "#### Enrich test data\n",
    "In the test_data.csv, each query is associated to a limited number of entities. There is a possibility that the query is also associated to other entities. Therefore, we need to enrich the test_data with any possible entities. This enriched data than named as 'silver data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed38992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "if 'queryTest' not in locals():\n",
    "    queryTest = {}    \n",
    "queryTest['noPredicate'] = {}\n",
    "\n",
    "def generateQueryTest(dfTest, name = None):\n",
    "    if name == None:\n",
    "        qTestIdx = str(len(queryTest['noPredicate']))\n",
    "    else:\n",
    "        qTestIdx = name\n",
    "    queryTest['noPredicate'][qTestIdx] = {}\n",
    "    for index, row in dfTest.iterrows():\n",
    "        query = row['query'] if row['check']==0 else row['query_modif']\n",
    "        classes = ast.literal_eval(row['classes']).keys()\n",
    "        if query not in queryTest['noPredicate'][qTestIdx]:\n",
    "            queryTest['noPredicate'][qTestIdx][query] = {'vars':[row['varID']], 'score':row['score']}\n",
    "        else:\n",
    "            queryTest['noPredicate'][qTestIdx][query]['vars'] += [row['varID']]\n",
    "        lstVars = [class2Vars[c] for c in classes]\n",
    "        otherVarIds = set.intersection(*map(set,lstVars))\n",
    "        queryTest['noPredicate'][qTestIdx][query]['vars'] += list(otherVarIds)\n",
    "        queryTest['noPredicate'][qTestIdx][query]['vars'] = list(set(queryTest['noPredicate'][qTestIdx][query]['vars']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the best test data\n",
    "generateQueryTest(df[(df.check==0)])\n",
    "# this is for the first and second best test data\n",
    "generateQueryTest(df[(df.check==0) | (df.check==-1)]) \n",
    "# this is for the first, second, and third best test data\n",
    "generateQueryTest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb29bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate query test with predicate\n",
    "\n",
    "def generateQueryPredicateTest(dfTest, name = None):\n",
    "    if name == None:\n",
    "        qTestIdx = str(len(queryTest['withPredicate']))\n",
    "    else:\n",
    "        qTestIdx = name\n",
    "    queryTest['withPredicate'][qTestIdx] = {}\n",
    "    for index, row in tqdm(dfTest.iterrows(), total=dfTest.shape[0]):\n",
    "        query = row['query'] if row['check']==0 else row['query_modif']\n",
    "        doc = nlp(query)\n",
    "        classes = list(entityKeys[row['varID']]['classes'].keys())\n",
    "        \n",
    "        clsPredicate = {}\n",
    "        for ent in doc.ents:\n",
    "            entClasses = list(getClasses(ent.text, topK=10).keys())\n",
    "            for classId in entClasses:\n",
    "                if classId in classes and classId not in clsPredicate and bool(random.getrandbits(1)):\n",
    "                    entPredicates = random.choice(entityKeys[row['varID']]['classes'][classId]['path'])\n",
    "                    if 'is' in entPredicates: entPredicates.remove('is')\n",
    "                    if len(entPredicates) > 0:\n",
    "                        # we select randomnly a predicate from position -3 to the end\n",
    "                        # the reason is that those predicates are more likely more relevant related to the ontology class\n",
    "                        entPredicate = random.choice(entPredicates[-3:])\n",
    "                        clsPredicate[classId] = entPredicate\n",
    "                        textPredicate = ' '.join(camel_case_split(entPredicate)).lower()\n",
    "                        textPredicate = textPredicate.replace('is ', '')                    \n",
    "                        query = query.replace(ent.text, '%s '%textPredicate+ent.text)\n",
    "                        break\n",
    "\n",
    "        if query not in queryTest['withPredicate'][qTestIdx]:\n",
    "            queryTest['withPredicate'][qTestIdx][query] = {'vars':[row['varID']], 'score':row['score']}\n",
    "        else:\n",
    "            queryTest['withPredicate'][qTestIdx][query]['vars'] += [row['varID']]\n",
    "        \n",
    "        lstVars = [class2Vars[c] for c in classes]\n",
    "        otherVarIds = list(set.intersection(*map(set,lstVars)))\n",
    "        \n",
    "        for varId in otherVarIds:\n",
    "            for classId, predicate in clsPredicate.items():\n",
    "                if predicate not in entityKeys[varId]['classes'][classId]['path']:\n",
    "                    otherVarIds.remove(varId)\n",
    "                    break\n",
    "\n",
    "        \n",
    "        queryTest['withPredicate'][qTestIdx][query]['vars'] += list(otherVarIds)\n",
    "        queryTest['withPredicate'][qTestIdx][query]['vars'] = list(set(queryTest['withPredicate'][qTestIdx][query]['vars']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some functions, we utilise scispacy to locate phrases or concepts related to a query. \n",
    "# For example a query 'Calcium reverse membrane potential.' \n",
    "# is identified having 2 concepts of 'calcium' and 'reverse membrane potential'\n",
    "# For accurate identification we use 'en_core_sci_scibert' which required GPU for faster performance\n",
    "\n",
    "import en_core_sci_scibert\n",
    "nlp = en_core_sci_scibert.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7999196",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTest['withPredicate'] = {}\n",
    "# this is for the best test data\n",
    "generateQueryPredicateTest(df[(df.check==0)])\n",
    "# this is for the first and second best test data\n",
    "generateQueryPredicateTest(df[(df.check==0) | (df.check==-1)]) \n",
    "# this is for the first, second, and third best test data\n",
    "generateQueryPredicateTest(df[(df.check==0) | (df.check==-1) | (df.check==-2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate combine test data\n",
    "import random\n",
    "listQueryP = random.sample(list(queryTest['withPredicate']['2']), len(queryTest['noPredicate']['2']))\n",
    "queryTestCombine = {**queryTest['noPredicate']['2'], **{key:queryTest['withPredicate']['2'][key] for key in listQueryP}}\n",
    "queryTest['combine'] = queryTestCombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdd46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the size of initial query test\n",
    "print('no predicate', len(queryTest['noPredicate']['2']))\n",
    "print('with predicate', len(queryTest['withPredicate']['2']))\n",
    "print('combination', len(queryTest['combine']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc055e4",
   "metadata": {},
   "source": [
    "### Prepare for IR with classifier (for retrieval purpose)\n",
    "We are experimenting the automatic use of predicate in our retrieval. The classifier decides its use.\n",
    "- We prepare the classifier dataset here\n",
    "- Then, run the classifier training in another script to get the model\n",
    "    - Follow this link: <a href=\"Train Query Classifier - PMR.ipynb\">Train Classifier for PMR</a>\n",
    "- Finally, using the generated model in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be514ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entitySearchCombine(query, topK=20, minSim=0.5, indexType='class', pathType=None):\n",
    "    \"\"\"\n",
    "    In this approach: combining entitySearch and entitySearchClass\n",
    "    1. Query is chunked into entities and classified into biomedical phrases and predicate\n",
    "    2. Connect predicate to biomedical phrase\n",
    "    3. If indexType is 'class', generate vector for biomedical phrase\n",
    "    4. If indexType is 'class_predicate', generate vector for biomedical phrase and predicate pair\n",
    "    5. Combine vectors becoming one vector using mean function, named it as local vector\n",
    "    6. Get vector of query named it as global vector\n",
    "    7. Combine local vector and global vector\n",
    "    8. Get similar entities using cosine similarity\n",
    "    9. Return topK result in descending\n",
    "    \"\"\"\n",
    "\n",
    "    ### Get local query embedding \n",
    "    \n",
    "    doc = nlp(query)\n",
    "    \n",
    "    ontoClasses = []\n",
    "    predicates = []\n",
    "    validClassPredicates = {}\n",
    "    offset2Class = {}\n",
    "    cScores = []\n",
    "    for ent in doc.ents:\n",
    "        predicateScores = getPredicates(ent.text, topK=1)\n",
    "        pScore = list(predicateScores.values())[0][1]\n",
    "        classScores = getClasses(ent.text, topK=1)\n",
    "        cScore = list(classScores.values())[0][1]\n",
    "        if cScore >= pScore:\n",
    "            cScores += [cScore]\n",
    "            ontoClasses += [ent]\n",
    "            for token in ent:\n",
    "                offset2Class[token.i] = ent\n",
    "        elif indexType =='class_predicate' and pScore >= minSim:\n",
    "            predicates += [ent]\n",
    "    \n",
    "    if len(ontoClasses) == 0: \n",
    "        ontoClasses = [doc]\n",
    "        cScores = [list(getClasses(query, topK=1).values())[0][1]]\n",
    "            \n",
    "            \n",
    "    # check the entities describe by predicate (usually predicate's child)\n",
    "    for ent in predicates:\n",
    "        for token in ent:\n",
    "            for child in token.children:\n",
    "                if child.i in offset2Class:\n",
    "                    idx = ontoClasses.index(offset2Class[child.i])\n",
    "                    if idx not in validClassPredicates:\n",
    "                        validClassPredicates[idx] = [ent]\n",
    "                    else:\n",
    "                        validClassPredicates[idx] += [ent]\n",
    "                    break # only consider the closest class. remove break if considering all classes\n",
    "                \n",
    "    if indexType=='class':\n",
    "        classEmbeddings = [sbert.encode(ent.text, convert_to_tensor=True) for ent in ontoClasses]\n",
    "        textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)\n",
    "    elif indexType=='class_predicate':\n",
    "        classEmbeddings = []\n",
    "        for i in range(len(ontoClasses)):\n",
    "            classEmbedding = sbert.encode(ontoClasses[i].text, convert_to_tensor=True)\n",
    "            if i in validClassPredicates:\n",
    "                predicateEmbeddings = [sbert.encode(ent.text, convert_to_tensor=True) for ent in validClassPredicates[i]]\n",
    "                pathEmbedding = alpha * torch.mean(torch.stack(predicateEmbeddings, dim=0), dim=0)\n",
    "                classEmbedding = torch.mean(torch.stack([classEmbedding, pathEmbedding], dim=0), dim=0)\n",
    "            classEmbeddings += [classEmbedding]\n",
    "        textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)                                \n",
    "        \n",
    "    ### Get global query embedding\n",
    "    textEmbeddingGlobal = sbert.encode(query, convert_to_tensor=True)\n",
    "    ### Combine global and local embedding\n",
    "    factor = sum(cScores)/len(cScores)\n",
    "    textEmbedding = torch.mean(torch.stack([textEmbeddingGlobal, factor * textEmbedding], dim=0), dim=0)       \n",
    "    \n",
    "    # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "    cosScores = util.pytorch_cos_sim(textEmbedding, entityEmbedding[indexType])[0]\n",
    "    topResults = torch.topk(cosScores, k=topK)\n",
    "    entities = {}\n",
    "    varIds = list(entityKeys.keys())\n",
    "    for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "        entities[varIds[idx]] = [rank, score.item(), entityKeys[varIds[idx]]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultCombineClass = getMAP(queryTestCombine, entitySearchCombine, topK=10, indexType='class')\n",
    "resultCombineClassPredicate = getMAP(queryTestCombine, entitySearchCombine, topK=10, indexType='class_predicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94953a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the query -1=neutral, 0=class, 1=class_predicate\n",
    "result = list(zip(resultCombineClass['MAP']['stat'].values(), resultCombineClassPredicate['MAP']['stat'].values()))\n",
    "for idx, (query, value) in enumerate(queryTestCombine.items()):\n",
    "    if result[idx][0] == result[idx][1]:\n",
    "        value['indexType'] = -1\n",
    "    elif result[idx][0] > result[idx][1]:\n",
    "        value['indexType'] = 0\n",
    "    elif result[idx][0] < result[idx][1]:\n",
    "        value['indexType'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289321ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('casbert_resources/pmr_classifier_data.json', 'w') as fp:\n",
    "    json.dump(queryTestCombine, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b60835",
   "metadata": {},
   "source": [
    "#### Divide into train, validation, and test data (proportion 4:3:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'queries':[], 'labels':[]}\n",
    "for q, v in queryTestCombine.items():\n",
    "    if v['indexType'] != -1:\n",
    "        data['queries'] += [q]\n",
    "        data['labels'] += [v['indexType']]\n",
    "        \n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_train, df_eval, df_test = np.split(df.sample(frac=1, random_state=0), [int(.3*len(df)), int(.6*len(df))])\n",
    "print(df_train.shape, df_eval.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTest['combine'] = {}\n",
    "for q, v in queryTestCombine.items():\n",
    "    if v['indexType'] == -1 or q in df_test['queries'].to_list():\n",
    "        queryTest['combine'][q] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save queryTest to file\n",
    "with open('casbert_resources/pmr_query_test.json', 'w') as fp:\n",
    "    json.dump(queryTest, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
