{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba3e9e5",
   "metadata": {},
   "source": [
    "## Preparing data and indexing to be used for experiment using OMEXBioModels\n",
    "- We are using a file consisting of BioModels annotations in RDF format\n",
    "- The annotation standard is using OMEX\n",
    "- This file is provided by TR&D2 Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b23ee7",
   "metadata": {},
   "source": [
    "## CREATING EMBEDDINGS FOR ENTITIES, PREDICATES, AND ONTOLOGY CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecede78c",
   "metadata": {},
   "source": [
    "#### Required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9337086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import io\n",
    "import json\n",
    "from sentence_transformers import util\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import urllib.request   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735512c6",
   "metadata": {},
   "source": [
    "#### Loading BioModels OMEX RDF file into RDFLib graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'casbert_resources/AllBioModelsOMEXRDF_latest.rdf'\n",
    "with io.open(filename, 'r', encoding='\"ISO-8859-1\"') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse(data=text, format='xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711331c",
   "metadata": {},
   "source": [
    "#### Extract triples of entity, predicate, and ontology classes from RDFLib graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPathToObjs(s, g):\n",
    "    pathToObjs = []\n",
    "    tmpObjPath = {o:[p] for p, o in g.predicate_objects(subject=s)}\n",
    "    while len(tmpObjPath) > 0:\n",
    "        objKeys = tmpObjPath.copy()\n",
    "        for o in objKeys:\n",
    "            children = list(g.predicate_objects(subject=o))\n",
    "            if len(children) == 0:\n",
    "                pathToObjs += [{'p': tmpObjPath[o], 'o': o}]\n",
    "            else:\n",
    "                for pred, obj in children:\n",
    "                    tmpObjPath[obj] = tmpObjPath[o] + [pred]\n",
    "            del tmpObjPath[o]\n",
    "    return pathToObjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entities, queries, ontology classes\n",
    "\n",
    "# initialisation:\n",
    "entities = {}  # subject:{'path':[paths], 'object':[objects], 'desc':desc}\n",
    "classes = {}   # {class1:{'num':num, 'onto':onto, 'text':[]}, class2:{'num':num, 'onto':onto, 'text':[]}}\n",
    "predicates = {}\n",
    "\n",
    "#get all entities / subjects, objects, tracks:\n",
    "for s, p, o in tqdm(g):\n",
    "    try:\n",
    "        predicates[p.n3().rsplit('/',1)[-1][:-1].rsplit('#',1)[-1]] = ''\n",
    "        if len(list(g.subject_predicates(object=s))) == 0:\n",
    "            entity = {'id':len(entities), 'subject':s.n3()[s.n3().rfind('/')+1:-1], 'path':{}, 'object':[], 'desc':''}\n",
    "            predicates_objs = getPathToObjs(s, g)\n",
    "            for predicates_obj in predicates_objs:\n",
    "                # if object is class ontology\n",
    "\n",
    "                if predicates_obj['o'].startswith('http') and 'doi' not in predicates_obj['o'] and 'biomodels.db' not in predicates_obj['o']:\n",
    "                    ontoType, classId = predicates_obj['o'].n3()[:-1].split('/')[-2:]\n",
    "                    if classId not in classes: classes[classId] = {'num':0, 'onto':ontoType, 'text':[]}\n",
    "                    classes[classId]['num'] += 1\n",
    "                    entity['object'] += [classId]\n",
    "\n",
    "                    # get paths\n",
    "                    if classId not in entity['path']: entity['path'][classId] = []\n",
    "                    path = [p.n3().rsplit('/',1)[-1][:-1].rsplit('#',1)[-1] for p in predicates_obj['p'] if not p.endswith('/is') and not p.endswith('/hasPhysicalDefinition')]\n",
    "                    entity['path'][classId] += [path]\n",
    "\n",
    "            entities[entity['subject']] = entity\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22720a46",
   "metadata": {},
   "source": [
    "#### Organising ontology classes in the type of ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb40099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ontologies:\n",
    "ontologies = {}\n",
    "statOnto = {}\n",
    "for classId, data in classes.items():\n",
    "    if data['onto'] not in ontologies: \n",
    "        ontologies[data['onto']] = []\n",
    "        statOnto[data['onto']] = 0\n",
    "    ontologies[data['onto']] += [classId]\n",
    "    statOnto[data['onto']] += data['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75393439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalised data. merge same ontology such as obo.pw and pw, obo.go and go\n",
    "for onto in list(statOnto.keys()):\n",
    "    if 'obo.' in onto:\n",
    "        ontoName = onto[onto.find('.')+1:]\n",
    "        if ontoName in ontologies:\n",
    "            ontologies[ontoName] += ontologies[onto]\n",
    "            statOnto[ontoName] += statOnto[onto]\n",
    "            del ontologies[onto]\n",
    "            del statOnto[onto]\n",
    "    if '.ref' in onto:\n",
    "        ontoName = onto[:onto.find('.')]\n",
    "        ontologies[ontoName] += ontologies[onto]\n",
    "        statOnto[ontoName] += statOnto[onto]\n",
    "        del ontologies[onto]\n",
    "        del statOnto[onto]\n",
    "    try:\n",
    "        if statOnto[onto] < 100:\n",
    "            del ontologies[onto]\n",
    "            del statOnto[onto]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c5460",
   "metadata": {},
   "source": [
    "#### Extracting ontology class terms\n",
    "We extract ontology class terms from: \n",
    "Uniprot, Taxonomy, Kegg.pathway, Reactome, CHEBI, GO, FMA, PR, \n",
    "CL, EFO, OBI, PW, MAMO, BTO, NCIT, OPB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uniprot data\n",
    "## uniprot\n",
    "\n",
    "for uni in tqdm(ontologies['uniprot']):\n",
    "    classId = uni.split('/')[-1]\n",
    "    try:\n",
    "        contents = urllib.request.urlopen(\"https://www.uniprot.org/uniprot/\"+classId+\".txt\").read()\n",
    "        contents = contents.decode('utf-8').split('\\n')\n",
    "        names = []\n",
    "        for content in contents:\n",
    "            if content.startswith('OS '):\n",
    "                break\n",
    "            if content.startswith('DE '):\n",
    "                cts = ''\n",
    "                if 'Full=' in content:\n",
    "                    cts = content[content.index('Full=')+5:]\n",
    "                if 'Short=' in content:\n",
    "                    cts = content[content.index('Short=')+6:]\n",
    "                if cts != '':\n",
    "                    names += [cts[:cts.index('{')] if '{' in cts else cts]\n",
    "        classes[classId]['text'] = names\n",
    "\n",
    "#         break\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get taxonomy data\n",
    "## taxonomy\n",
    "for uni in tqdm(ontologies['taxonomy']):\n",
    "    classId = uni.split('/')[-1]\n",
    "    try:\n",
    "        contents = urllib.request.urlopen(\"https://www.uniprot.org/taxonomy/\"+classId+\".rdf\").read()\n",
    "        contents = contents.decode('utf-8').split('\\n')\n",
    "        names = []\n",
    "        for content in contents:\n",
    "            if content.startswith('<commonName>') or content.startswith('<scientificName>') or content.startswith('<otherName>') or content.startswith('<synonym>'):\n",
    "                names += [content[content.find('>')+1:content.rfind('<')]]\n",
    "        classes[classId]['text'] = list(set(names))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get kegg.pathway data\n",
    "## kegg.pathway\n",
    "for uni in tqdm(ontologies['kegg.pathway']):\n",
    "    classId = uni.split('/')[-1]\n",
    "    classes[classId]['text'] = []\n",
    "    try:\n",
    "        contents = urllib.request.urlopen(\"http://rest.kegg.jp/get/\"+classId).read()\n",
    "        contents = contents.decode('utf-8').split('\\n')\n",
    "        names = []\n",
    "        for content in contents:\n",
    "            if content.startswith('NAME'):\n",
    "                names += [content.split('NAME')[-1].strip()]\n",
    "                break\n",
    "        classes[classId]['text'] = names        \n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21995be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reactome data\n",
    "## reactome\n",
    "for uni in tqdm(ontologies['reactome']):\n",
    "    classId = uni.split('/')[-1]\n",
    "    classes[classId]['text'] = []\n",
    "    try:\n",
    "        contents = urllib.request.urlopen(\"https://reactome.org/ContentService/data/query/\"+classId).read()\n",
    "        contents = json.loads(contents)\n",
    "        names = contents['name']\n",
    "        names += [contents['speciesName']] if 'speciesName' in contents else []\n",
    "        classes[classId]['text'] = names        \n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d752f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onto_bio_portal(linkTemplate, ontoName, lower=False, replace=('',''), preff=''):\n",
    "    for uni in tqdm(ontologies[ontoName][:5]):\n",
    "        try:\n",
    "            classId = uni\n",
    "            if lower: \n",
    "                classId=classId.lower()\n",
    "            classId=classId.replace(replace[0],replace[1])\n",
    "            classId = preff + classId\n",
    "            link = linkTemplate.replace('$classId$', classId)\n",
    "#             print(link)\n",
    "            contents = urllib.request.urlopen(link).read()\n",
    "            contents = json.loads(contents)\n",
    "            classes[uni]['text'] = [contents['prefLabel']] + [s in contents['synonym']]\n",
    "        except:\n",
    "            print(uni)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f1c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/CHEBI/classes/http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'chebi',False, (':','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0196398",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/GO/classes/http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'go',False, (':','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/FMA/classes/http%3A%2F%2Fpurl.org%2Fsig%2Font%2Ffma%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'fma', True, (':',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e37390",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/PR/classes/http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'pr',False, (':','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245df00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/CL/classes/http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'cl',False, (':','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/EFO/classes/http%3A%2F%2Fwww.ebi.ac.uk%2Fefo%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'efo', preff='EFO_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e20d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/OBI/classes/http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'obi',False, (':','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99053f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/PW/classes/http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'pw',False, (':','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234273be",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/MAMO/classes/http%3A%2F%2Fidentifiers.org%2Fmamo%2F$classId$?apikey=fc5d5241-1e8e-4b44-b401-310ca39573f6'\n",
    "onto_bio_portal(linkTemplate, 'mamo',False, (':','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/BTO/classes/http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'bto',False, (':','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/NCIT/classes/http%3A%2F%2Fncicb.nci.nih.gov%2Fxml%2Fowl%2FEVS%2FThesaurus.owl%23$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'ncit',False, ('NCIT:',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65763db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTemplate = 'https://data.bioontology.org/ontologies/OPB/classes/http%3A%2F%2Fbhi.washington.edu%2FOPB%23$classId$?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb'\n",
    "onto_bio_portal(linkTemplate, 'opb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_resources/omex_classes.json', 'w') as fp:\n",
    "    json.dump(classes, fp)\n",
    "    \n",
    "with open('casbert_resources/omex_entities.json', 'w') as fp:\n",
    "    json.dump(entities,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0f69a",
   "metadata": {},
   "source": [
    "#### CREATE CLASS EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTModel = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "_model = SentenceTransformer(BERTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c579d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classId in tqdm(list(classes)):\n",
    "    if len(classes[classId]['text']) > 0:\n",
    "        classes[classId]['embedding'] = torch.mean(_model.encode(classes[classId]['text'], convert_to_tensor=True), dim=0)\n",
    "    else:\n",
    "        del classes[classId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29732653",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(predicates, 'casbert_resources/omex_predicates.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac7656",
   "metadata": {},
   "source": [
    "#### CREATE PREDICATE EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c90db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "for p in tqdm(predicates):\n",
    "    predicates[p] = _model.encode(' '.join(camel_case_split(p)), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(predicates, 'casbert_resources/omex_predicates.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6adf6e",
   "metadata": {},
   "source": [
    "#### CREATE ENTITY EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "entityEmbeddings = {'entityIds':[], 'class':[], 'class_predicate':[]}\n",
    "alpha = 0.22\n",
    "for entityId, entity in tqdm(entities.items()):\n",
    "    if len(entity['object']) == 0: continue\n",
    "    objects = [classes[obj]['embedding'] for obj in entity['object'] if obj in classes]\n",
    "    entityEmbeddings['class'] += [torch.mean(torch.stack(objects), dim=0)]\n",
    "    entityEmbeddings['entityIds'] += [entityId]\n",
    "    pathEmbeddings = []\n",
    "    for obj, paths in entity['path'].items():\n",
    "        if obj not in classes: continue\n",
    "        objEmbedding = classes[obj]['embedding']\n",
    "        try:\n",
    "            trackEmbedding = torch.mean(torch.stack([torch.mean(torch.stack([predicates[p] for p in path]), dim=0) for path in paths]), dim=0)\n",
    "            pathEmbeddings += [torch.mean(torch.stack([objEmbedding + alpha * trackEmbedding]), dim=0)]\n",
    "        except:\n",
    "            pathEmbeddings += [objEmbedding]\n",
    "    entityEmbeddings['class_predicate'] += [torch.mean(torch.stack(pathEmbeddings), dim=0)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "entityEmbeddings['class'] = torch.stack(entityEmbeddings['class'])\n",
    "entityEmbeddings['class_predicate'] = torch.stack(entityEmbeddings['class_predicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24574d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(entityEmbeddings, 'casbert_resources/omex_entities.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f1259",
   "metadata": {},
   "source": [
    "### Performance measure using Mean average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagePrecision(prediction):\n",
    "    if 1 not in prediction:\n",
    "        return 0\n",
    "    tot = 0\n",
    "    for idx, p in enumerate(prediction):\n",
    "        if p>0:\n",
    "            tot += sum(prediction[0:idx+1])/(idx+1)\n",
    "    return tot/sum(prediction)\n",
    "\n",
    "def meanAP(predictions):\n",
    "    tap = 0\n",
    "    stat = {}\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        ap = averagePrecision(prediction)\n",
    "        stat[idx] = ap\n",
    "        tap += ap\n",
    "    return {'score':tap/len(predictions), 'stat':stat}\n",
    "\n",
    "def meanRR(predictions):\n",
    "    trr = 0\n",
    "    stat = {}\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        rr = 1/(prediction.index(1)+1) if 1 in prediction else 0\n",
    "        stat[idx] = rr\n",
    "        trr += rr\n",
    "    return {'score':trr/len(predictions), 'stat':stat}\n",
    "\n",
    "def getMAP(queries, searchFunction, indexType=None, pathType=None, topK=10, minSim=0.5):\n",
    "    predictions = []\n",
    "    for query, facts in tqdm(queries.items()):\n",
    "        results = searchFunction(query=query, topK=topK, indexType=indexType, pathType=pathType, minSim=minSim)\n",
    "        predictions += [[1 if varId in facts['vars'] else 0 for varId in results]]\n",
    "    MAP = meanAP(predictions)\n",
    "    return {'MAP':MAP,'MRR':meanRR(predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5f987",
   "metadata": {},
   "source": [
    "### CREATING QUERY _ ENTITY SET FOR EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e31b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get queries\n",
    "# get entities, queries, ontology classes\n",
    "\n",
    "# initialisation:\n",
    "queries = {}   # {'query1':{'vars':entityIds, 'score':score}, 'query2':{'vars':entityIds, 'score':score}}\n",
    "qRes = []\n",
    "\n",
    "#get all entities / subjects, objects, tracks:\n",
    "for s, p, o in tqdm(g):\n",
    "    if len(list(g.subject_predicates(object=s))) == 0:\n",
    "        predicates_objs = getPathToObjs(s, g)\n",
    "        for predicates_obj in predicates_objs:\n",
    "            if predicates_obj['p'][-1].endswith('description') and len(predicates_obj['o']) < 200:\n",
    "                q = predicates_obj['o'].n3()\n",
    "                while q[0] == '\"': q = q[1:]\n",
    "                while q[-1] == '\"': q = q[:-1]\n",
    "                q = \" \".join(q.split())    \n",
    "                try:\n",
    "                    q = re.search(r'(?<=>).*(?=<)' , q).group(0)\n",
    "                except:\n",
    "                    pass\n",
    "                if q not in queries: \n",
    "                    queries[q] = {'vars':[], 'score':0}\n",
    "                entId = s.n3()[s.n3().rfind('/')+1:-1]\n",
    "                if entId in entityEmbeddings['entityIds']:\n",
    "                    qEmbedding = _model.encode(q, convert_to_tensor=True)\n",
    "                    pos = entityEmbeddings['entityIds'].index(entId)\n",
    "                    score = util.pytorch_cos_sim(qEmbedding, entityEmbeddings['class'][pos])\n",
    "                    queries[q]['vars'] += [entId]\n",
    "                    if score > queries[q]['score']:\n",
    "                        queries[q]['score'] = score\n",
    "                    qRes += [s.n3()[s.n3().rfind('/')+1:-1]]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPathToObjs2(s, g):\n",
    "    pathToObjs = []\n",
    "    tmpObjPath = {o:[(s,p)] for p, o in g.predicate_objects(subject=s)}\n",
    "    while len(tmpObjPath) > 0:\n",
    "        objKeys = tmpObjPath.copy()\n",
    "        for o in objKeys:\n",
    "            children = list(g.predicate_objects(subject=o))\n",
    "            if len(children) == 0:\n",
    "                pathToObjs += [{'p': tmpObjPath[o], 'o': o}]\n",
    "            else:\n",
    "                for pred, obj in children:\n",
    "                    tmpObjPath[obj] = tmpObjPath[o] + [(o,pred)]\n",
    "            del tmpObjPath[o]\n",
    "    return pathToObjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get queries\n",
    "# get entities, queries, ontology classes\n",
    "\n",
    "# initialisation:\n",
    "queries = {}   # {'query1':{'vars':entityIds, 'score':score}, 'query2':{'vars':entityIds, 'score':score}}\n",
    "qRes = []\n",
    "\n",
    "#get all entities / subjects, objects, tracks:\n",
    "for s, p, o in tqdm(g):\n",
    "    if len(list(g.subject_predicates(object=s))) == 0:\n",
    "        predicates_objs = getPathToObjs2(s, g)\n",
    "        for predicates_obj in predicates_objs:\n",
    "            if not predicates_obj['o'].startswith('http') and len(predicates_obj['o'])<200:\n",
    "                if predicates_obj['o'] in predicates_obj['p'][-1][0]:\n",
    "                    continue\n",
    "                \n",
    "                q = predicates_obj['o'].n3()\n",
    "                while q[0] == '\"': q = q[1:]\n",
    "                while q[-1] == '\"': q = q[:-1]\n",
    "                q = \" \".join(q.split())    \n",
    "                try:\n",
    "                    q = re.search(r'(?<=>).*(?=<)' , q).group(0)\n",
    "                except:\n",
    "                    pass\n",
    "                if q not in queries: \n",
    "                    queries[q] = {'vars':[], 'score':0}\n",
    "                entId = s.n3()[s.n3().rfind('/')+1:-1]\n",
    "                if entId in entityEmbeddings['entityIds']:\n",
    "                    qEmbedding = _model.encode(q, convert_to_tensor=True)\n",
    "                    pos = entityEmbeddings['entityIds'].index(entId)\n",
    "                    score = util.pytorch_cos_sim(qEmbedding, entityEmbeddings['class'][pos]).item()\n",
    "                    queries[q]['vars'] += [entId]\n",
    "                    if score > queries[q]['score']:\n",
    "                        queries[q]['score'] = score\n",
    "                    qRes += [s.n3()[s.n3().rfind('/')+1:-1]]\n",
    "                \n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in queries:\n",
    "    queries[q]['vars'] = list(set(queries[q]['vars']))\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11622ecc",
   "metadata": {},
   "source": [
    "#### Filter queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## - remove queries with similarity score to the entities are 0\n",
    "## - removing entity id in queries that do not have embedding\n",
    "## - remove query that do not have variable\n",
    "\n",
    "\n",
    "count = 0\n",
    "for q in tqdm(list(queries.keys())):\n",
    "    if queries[q]['score'] == 0:\n",
    "        del queries[q]\n",
    "#         continue\n",
    "#     queries[q]['vars'] = [entityId for entityId in queries[q]['vars'] if entityId in entityEmbeddings['entityIds']]\n",
    "#     if len(queries[q]['vars']) == 0:\n",
    "#         del queries[q]\n",
    "        count += 1\n",
    "\n",
    "print('deleted queries: ', count, '; number of candidate queries: ', len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## - remove queries containing number only\n",
    "for q in list(queries.keys()):\n",
    "    test = q.replace(\".\", \"\", 1)\n",
    "    if test.isdigit():\n",
    "        del queries[q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65812cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## - remove queries containing variable name pattern only\n",
    "\n",
    "for q in list(queries.keys()):\n",
    "                    \n",
    "    status = any([q.replace(\"v\", \"\", 1).isdigit(), # v1, v2, v3, ...\n",
    "                 q.replace(\"reaction_\", \"\", 1).isdigit(), # reaction_1, reaction_2, ...\n",
    "                 q.replace(\"r\", \"\", 1).isdigit(), # r1, r2, r3, ...\n",
    "                 q.replace(\"R\", \"\", 1).isdigit(), # R1, R2, R3, ...\n",
    "                 q.replace(\"rel\", \"\", 1).isdigit(), # rel1, rel2, rel3, ...\n",
    "                 q.replace(\"re\", \"\", 1).isdigit(), # re1, re2, re3, ...\n",
    "                 q.replace(\"R_\", \"\", 1).isdigit(), # R_1, R_2, R_3, ...\n",
    "                 ])\n",
    "    \n",
    "    if status:\n",
    "        del queries[q]\n",
    "        \n",
    "    if q.lower().startswith('compartment'):\n",
    "        del queries[q]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f50fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select queries with similarity >=0.4\n",
    "for q in list(queries.keys()):\n",
    "    if queries[q]['score'] < 0.5:\n",
    "        del queries[q]\n",
    "        \n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36987189",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing queries with too many entities since it cannot represent MAP and MRR well\n",
    "for q in list(queries.keys()):\n",
    "    if len(queries[q]['vars']) > 50:\n",
    "        del queries[q]\n",
    "        \n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d955a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing queries containing only one ontology class concept\n",
    "for q in tqdm(list(queries.keys())):\n",
    "    if len(nlp(q).ents) == 1:\n",
    "        del queries[q]\n",
    "        \n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0aeac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove entities related to queries where the number of ontology classes smaller than the number of concept in the query\n",
    "## if there is no entity related to the query, the query is deleted\n",
    "for q in tqdm(list(queries.keys())):\n",
    "\n",
    "    v = queries[q]\n",
    "    # get ontology class concept\n",
    "    concepts = {}\n",
    "    for ent in nlp(q).ents:\n",
    "        entClasses = getClasses(ent.text, topK=10)\n",
    "        for classId, classDef in entClasses.items():\n",
    "            if classDef[1] < 0.6:\n",
    "                break\n",
    "            if ent not in concepts:\n",
    "                concepts[ent] = []\n",
    "            concepts[ent] += [classId]\n",
    "    \n",
    "    # delete short entities\n",
    "    for entId in copy.deepcopy(v['vars']):\n",
    "        if len(concepts) == 0: continue\n",
    "        if len(entities[entId]['object']) / len(concepts) < 0.6:\n",
    "            v['vars'].remove(entId)\n",
    "            \n",
    "    # delete queries with no entity\n",
    "    if len(v['vars']) == 0:\n",
    "        del queries[q]\n",
    "\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79688f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove queries with no relevancies between query and entities\n",
    "\n",
    "predictions = {}\n",
    "for query in tqdm(list(queries.keys())):\n",
    "    results = entitySearch(query=query)\n",
    "    predictions[query] = averagePrecision([1 if varId in queries[query]['vars'] else 0 for varId in results])\n",
    "    if predictions[query] == 0:\n",
    "        del queries[query]\n",
    "    \n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a79a5",
   "metadata": {},
   "source": [
    "#### Enrich test data\n",
    "In the test_data.csv, each query is associated to a limited number of entities. There is a possibility that the query is also associated to other entities. Therefore, we need to enrich the test_data with any possible entities. This enriched data than named as 'silver data'\n",
    "\n",
    "##### Create dictionary ontology class to variables\n",
    "It is used to enrich test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class2Vars = {}\n",
    "\n",
    "for varId in entityEmbedding['entityIds']:\n",
    "    value = entities[varId]\n",
    "    for classId in value['object']:\n",
    "        if classId not in class2Vars: class2Vars[classId] = []\n",
    "        class2Vars[classId] += [varId]             \n",
    "\n",
    "# enrich with similar entities\n",
    "def enrichQueryWithOtherEntities(dictQueries):\n",
    "    for q, v in tqdm(dictQueries.items()):\n",
    "        newVarIds = []\n",
    "        for varId in v['vars']:\n",
    "            newVarIds += [varId]\n",
    "            lstVars = [class2Vars[c] for c in entities[varId]['object']]\n",
    "            otherVarIds = list(set.intersection(*map(set,lstVars)))\n",
    "            newVarIds += otherVarIds\n",
    "\n",
    "        dictQueries[q]['vars'] = list(set(dictQueries[q]['vars'] + newVarIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichQueryWithOtherEntities(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e771ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich with entities having similar pattern with queries\n",
    "import itertools\n",
    "for q in tqdm(list(queries.keys())):\n",
    "    \n",
    "    # get ontology class concept\n",
    "    concepts = {}\n",
    "    for ent in nlp(q).ents:\n",
    "        entClasses = getClasses(ent.text, topK=10)\n",
    "        for classId, classDef in entClasses.items():\n",
    "            if ent not in concepts:\n",
    "                concepts[ent] = []\n",
    "            if classDef[1] < 0.6: break\n",
    "            concepts[ent] += [classId]\n",
    "    \n",
    "    # check each concept\n",
    "    candEntities = []\n",
    "    for concept, classIds in concepts.items():\n",
    "        eIds = [class2Vars[classId] for classId in classIds if classId in class2Vars]\n",
    "        candEntities += [list(set(itertools.chain.from_iterable(eIds)))]\n",
    "    if len(candEntities) > 0:\n",
    "        newEntities = list(set.intersection(*map(set,candEntities)))\n",
    "        queries[q]['vars'] = list(set(queries[q]['vars'] + newEntities))\n",
    "\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTest = {'noPredicate':queries, 'withPredicate':{}, 'combine':{}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de43ac4",
   "metadata": {},
   "source": [
    "#### Generate query test with predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTest['withPredicate'] = {}\n",
    "for q, variables in tqdm(queryTest['noPredicate'].items()):\n",
    "    doc = nlp(q)\n",
    "    entToClasses = {}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        entToClasses[ent] = list(getClasses(ent.text, topK=10).keys())\n",
    "        \n",
    "    for entityId in variables['vars']:\n",
    "        _classes = entities[entityId]['path']\n",
    "        clsPredicate = {}\n",
    "        query = q\n",
    "        for entClasses in entToClasses.values():\n",
    "            for classId in entClasses:\n",
    "                if classId in _classes and classId not in clsPredicate and bool(random.getrandbits(1)):\n",
    "                    entPredicates = random.choice(_classes[classId])\n",
    "                    if len(entPredicates) > 0:\n",
    "                        # we select randomnly a predicate from position -3 to the end\n",
    "                        # the reason is that those predicates are more likely more relevant related to the ontology class\n",
    "                        entPredicate = random.choice(entPredicates[-3:])\n",
    "                        clsPredicate[classId] = entPredicate\n",
    "                        textPredicate = ' '.join(camel_case_split(entPredicate)).lower()\n",
    "                        textPredicate = textPredicate.replace('is ', '')                    \n",
    "                        query = query.replace(ent.text, '%s '%textPredicate+ent.text)\n",
    "                        break\n",
    "        \n",
    "        if query == q: continue\n",
    "        if query not in queryTest['withPredicate']:\n",
    "            queryTest['withPredicate'][query] = {'vars':[entityId], 'score':variables['score']}\n",
    "        else:\n",
    "            queryTest['withPredicate'][query]['vars'] += [entityId]\n",
    "        \n",
    "        otherVarIds = copy.deepcopy(variables['vars'])\n",
    "        \n",
    "        for varId in otherVarIds:\n",
    "            for classId, predicate in clsPredicate.items():\n",
    "                if classId not in entities[varId]['path']:\n",
    "                    otherVarIds.remove(varId)\n",
    "                    break\n",
    "                if all([1 if predicate not in path else 0 for path in entities[varId]['path'][classId]]):\n",
    "                    otherVarIds.remove(varId)\n",
    "                    break\n",
    "        \n",
    "        queryTest['withPredicate'][query]['vars'] += list(otherVarIds)\n",
    "        queryTest['withPredicate'][query]['vars'] = list(set(queryTest['withPredicate'][query]['vars']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a9b76",
   "metadata": {},
   "source": [
    "#### Prepare for IR with classifier\n",
    "We are experimenting the automatic use of predicate in our retrieval. The classifier decides its use.\n",
    "- We prepare the classifier dataset here\n",
    "- Then, run the classifier training in another script to get the model\n",
    "    - Follow this link: <a href=\"Train Query Classifier - OMEX.ipynb\">Train Classifier for OMEX BioModels</a>\n",
    "- Finally, using the generated model in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entitySearchCombine(query, topK=20, minSim=0.5, indexType='class', pathType=None):\n",
    "    \"\"\"\n",
    "    In this approach: combining entitySearch and entitySearchClass\n",
    "    1. Query is chunked into entities and classified into biomedical phrases and predicate\n",
    "    2. Connect predicate to biomedical phrase\n",
    "    3. If indexType is 'class', generate vector for biomedical phrase\n",
    "    4. If indexType is 'class_predicate', generate vector for biomedical phrase and predicate pair\n",
    "    5. Combine vectors becoming one vector using mean function, named it as local vector\n",
    "    6. Get vector of query named it as global vector\n",
    "    7. Combine local vector and global vector\n",
    "    8. Get similar entities using cosine similarity\n",
    "    9. Return topK result in descending\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get local query embedding \n",
    "    \n",
    "    doc = nlp(query)\n",
    "    alpha = 1\n",
    "    \n",
    "    ontoClasses = []\n",
    "    predicates = []\n",
    "    validClassPredicates = {}\n",
    "    offset2Class = {}\n",
    "    cScores = []\n",
    "    for ent in doc.ents:\n",
    "        predicateScores = getPredicates(ent.text, topK=1)\n",
    "        pScore = list(predicateScores.values())[0][1]\n",
    "        classScores = getClasses(ent.text, topK=1)\n",
    "        cScore = list(classScores.values())[0][1]\n",
    "        if cScore >= pScore:\n",
    "            cScores += [cScore]\n",
    "            ontoClasses += [ent]\n",
    "            for token in ent:\n",
    "                offset2Class[token.i] = ent\n",
    "        elif indexType =='class_predicate' and pScore >= minSim:\n",
    "            predicates += [ent]\n",
    "    \n",
    "    if len(ontoClasses) == 0: \n",
    "        ontoClasses = [doc]\n",
    "        cScores = [list(getClasses(query, topK=1).values())[0][1]]\n",
    "            \n",
    "    # check the entities describe by predicate (usually predicate's child)\n",
    "    for ent in predicates:\n",
    "        for token in ent:\n",
    "            for child in token.children:\n",
    "                if child.i in offset2Class:\n",
    "                    idx = ontoClasses.index(offset2Class[child.i])\n",
    "                    if idx not in validClassPredicates:\n",
    "                        validClassPredicates[idx] = [ent]\n",
    "                    else:\n",
    "                        validClassPredicates[idx] += [ent]\n",
    "                    break # only consider the closest class. remove break if considering all classes\n",
    "                \n",
    "    if indexType=='class':\n",
    "        classEmbeddings = [_model.encode(ent.text, convert_to_tensor=True) for ent in ontoClasses]\n",
    "        textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)\n",
    "    elif indexType=='class_predicate':\n",
    "        classEmbeddings = []\n",
    "        for i in range(len(ontoClasses)):\n",
    "            classEmbedding = _model.encode(ontoClasses[i].text, convert_to_tensor=True)\n",
    "            if i in validClassPredicates:\n",
    "                predicateEmbeddings = [_model.encode(ent.text, convert_to_tensor=True) for ent in validClassPredicates[i]]\n",
    "                pathEmbedding = alpha * torch.mean(torch.stack(predicateEmbeddings, dim=0), dim=0)\n",
    "                classEmbedding = torch.mean(torch.stack([classEmbedding, pathEmbedding], dim=0), dim=0)\n",
    "            classEmbeddings += [classEmbedding]\n",
    "        textEmbedding = torch.mean(torch.stack(classEmbeddings, dim=0), dim=0)                                \n",
    "        \n",
    "    ### Get global query embedding\n",
    "    textEmbeddingGlobal = _model.encode(query, convert_to_tensor=True)\n",
    "    ### Combine global and local embedding\n",
    "    factor = sum(cScores)/len(cScores)\n",
    "    textEmbedding = torch.mean(torch.stack([textEmbeddingGlobal, factor * textEmbedding], dim=0), dim=0)       \n",
    "    \n",
    "    # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "    cosScores = util.pytorch_cos_sim(textEmbedding, entityEmbeddings[indexType])[0]\n",
    "    topResults = torch.topk(cosScores, k=topK)\n",
    "    results = {}\n",
    "    varIds = entityEmbeddings['entityIds']\n",
    "    for rank, (score, idx) in enumerate(zip(topResults[0], topResults[1])):\n",
    "        results[varIds[idx]] = [rank, score.item(), entities[varIds[idx]]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef509b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTestCombine = {**queryTest['noPredicate'], **queryTest['withPredicate']}\n",
    "\n",
    "resultCombineClass = getMAP(queryTestCombine, entitySearchCombine, topK=10, indexType='class')\n",
    "resultCombineClassPredicate = getMAP(queryTestCombine, entitySearchCombine, topK=10, indexType='class_predicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the query -1=neutral, 0=class, 1=class_predicate\n",
    "result = list(zip(resultCombineClass['MAP']['stat'].values(), resultCombineClassPredicate['MAP']['stat'].values()))\n",
    "# r = [max(d) for d in r]\n",
    "c1, c2, c3 = 0, 0, 0\n",
    "for idx, (query, value) in enumerate(queryTestCombine.items()):\n",
    "    if result[idx][0] == result[idx][1]:\n",
    "        value['indexType'] = -1\n",
    "        c1+=1\n",
    "    elif result[idx][0] > result[idx][1]:\n",
    "        value['indexType'] = 0\n",
    "        c2+=1\n",
    "    elif result[idx][0] < result[idx][1]:\n",
    "        value['indexType'] = 1\n",
    "        c3+=1\n",
    "\n",
    "print(c1,c2,c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edf48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('casbert_resources/omex_classifier_data.json', 'w') as fp:\n",
    "    json.dump(queryTestCombine, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140a77e",
   "metadata": {},
   "source": [
    "##### Divide into train, validation, and test data (proportion 4:3:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a96ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'queries':[], 'labels':[]}\n",
    "for q, v in queryTestCombine.items():\n",
    "    if v['indexType'] != -1:\n",
    "        data['queries'] += [q]\n",
    "        data['labels'] += [v['indexType']]\n",
    "        \n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab709a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_train, df_eval, df_test = np.split(df.sample(frac=1, random_state=0), [int(.3*len(df)), int(.6*len(df))])\n",
    "print(df_train.shape, df_eval.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTest['combine'] = {}\n",
    "for q, v in queryTestCombine.items():\n",
    "    if v['indexType'] == -1 or q in df_test['queries'].to_list():\n",
    "        queryTest['combine'][q] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1daf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save queryTest to file\n",
    "with open('casbert_resources/omex_queryTest.json', 'w') as fp:\n",
    "    json.dump(queryTest, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106bb16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
